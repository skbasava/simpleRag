
import re


async def resolve_chip(self, chip_input: str) -> dict | None:
    """
    Resolve chip based on:
      - base family (kaanapali)
      - optional revision (r2)

    Examples:
      kaanapali        -> base chip
      kaanapali r2     -> revision chip
    """

    if not chip_input:
        return None

    raw = chip_input.lower().strip()

    # ------------------------------------
    # 1. Extract revision (r2, r3, etc.)
    # ------------------------------------
    revision_match = re.search(r"\br\d+\b", raw)
    revision = revision_match.group(0) if revision_match else None

    # Remove revision from base search string
    base_name = re.sub(r"\br\d+\b", "", raw).strip()

    # ------------------------------------
    # 2. Fetch chip list from API
    # ------------------------------------
    resp = self.request(
        "GET",
        f"{self.base_url}/api/1/chip/",
    )

    chips = resp.json()

    if not chips:
        return None

    # ------------------------------------
    # 3. Match by family token
    # ------------------------------------
    family_matches = []

    for chip in chips:
        full_name = chip["name"].lower()

        # Extract family inside parentheses
        match = re.search(r"\((.*?)\)", full_name)
        family = match.group(1).lower() if match else ""

        if base_name == family or base_name in full_name:
            family_matches.append(chip)

    if not family_matches:
        return None

    # ------------------------------------
    # 4. If revision requested → match exact
    # ------------------------------------
    if revision:
        for chip in family_matches:
            if revision in chip["name"].lower():
                return chip

        return None  # revision explicitly requested but not found

    # ------------------------------------
    # 5. No revision → pick base (no rX)
    # ------------------------------------
    for chip in family_matches:
        if not re.search(r"\br\d+\b", chip["name"].lower()):
            return chip

    # Fallback: return first match
    return family_matches[0]




import asyncio
import json
from typing import Dict, Any
from concurrent.futures import ThreadPoolExecutor

from backend.redis_keys import (
    MPU_KEY,
    INGEST_DONE_KEY,
    INGEST_LOCK_KEY,
)
from backend.xml_parser import extract_mpus, ingest_single_mpu


EXECUTOR = ThreadPoolExecutor(max_workers=8)
LOCK_TTL = 600
DONE_TTL = 36 * 3600


class PolicyOrchestrator:
    def __init__(self, client, redis):
        self.client = client
        self.redis = redis

    # =====================================================
    # PUBLIC ENTRY
    # =====================================================

    async def get_policy_by_mpu(
        self,
        chip_name: str,
        version: str | None,
        mpu_name: str,
    ) -> Dict[str, Any]:
        """
        Main API entry.
        """

        # 1️⃣ Resolve chip via API (NOT Redis)
        chip = await self.client.get_chip_by_name(chip_name)
        if not chip:
            raise ValueError(f"Chip not found: {chip_name}")

        chip_id = chip["id"]

        # 2️⃣ Resolve policy via API
        policy = await self.client.get_policy_for_chip(
            chip_id=chip_id,
            version=version,
        )

        policy_version = policy["version"]

        # 3️⃣ Ensure XML ingestion
        await self._ensure_ingested(chip_id, policy_version)

        # 4️⃣ Fetch MPU from Redis
        key = MPU_KEY.format(
            chip_id=chip_id,
            policy_version=policy_version,
            mpu_name=mpu_name.upper(),
        )

        raw = self.redis.get(key)
        if not raw:
            raise KeyError(f"MPU not found: {mpu_name}")

        return json.loads(raw)

    # =====================================================
    # INGESTION ORCHESTRATION
    # =====================================================

    async def _ensure_ingested(self, chip_id: int, policy_version: str):
        done_key = INGEST_DONE_KEY.format(
            chip_id=chip_id,
            policy_version=policy_version,
        )

        lock_key = INGEST_LOCK_KEY.format(
            chip_id=chip_id,
            policy_version=policy_version,
        )

        # Fast path
        if self.redis.exists(done_key):
            return

        # Acquire lock
        if not self.redis.set(lock_key, 1, nx=True, ex=LOCK_TTL):
            return

        try:
            await self._ingest_chip(chip_id, policy_version)
            self.redis.set(done_key, 1, ex=DONE_TTL)
        finally:
            self.redis.delete(lock_key)

    async def _ingest_chip(self, chip_id: int, policy_version: str):
        """
        Fetch XML → Parse MPUs → Store each MPU
        """

        async for xml_text in self.client.fetch_policy_xml(
            chip_id,
            policy_version,
        ):
            mpus = extract_mpus(xml_text)

            loop = asyncio.get_running_loop()
            tasks = [
                loop.run_in_executor(
                    EXECUTOR,
                    ingest_single_mpu,
                    xml_text,
                    chip_id,
                    policy_version,
                    mpu,
                    self.redis,
                )
                for mpu in mpus
            ]

            for t in asyncio.as_completed(tasks):
                await t



import json
import xml.etree.ElementTree as ET
from backend.redis_keys import MPU_KEY


def extract_mpus(xml_text: str):
    root = ET.fromstring(xml_text)
    return [m.attrib["name"] for m in root.findall(".//MPU")]


def extract_hw_params(mpu_elem):
    result = {}

    hw = mpu_elem.find("HwConfig")
    if hw is None:
        return result

    design = hw.find("HwDesignParameters")
    if design is not None:
        result["design"] = {
            p.attrib["name"]: p.attrib["value"]
            for p in design.findall("Parameter")
        }

    integration = hw.find("HwIntegrationParameters")
    if integration is not None:
        result["integration"] = {
            p.attrib["name"]: p.attrib["value"]
            for p in integration.findall("Parameter")
        }

    return result


def ingest_single_mpu(
    xml_text: str,
    chip_id: int,
    policy_version: str,
    mpu_name: str,
    redis_client,
):
    root = ET.fromstring(xml_text)

    mpu_elem = root.find(f".//MPU[@name='{mpu_name}']")
    if mpu_elem is None:
        return

    hw = extract_hw_params(mpu_elem)

    regions = []
    for region in mpu_elem.findall(".//Region"):
        regions.append({
            "id": region.attrib.get("id"),
            "raw": ET.tostring(region, encoding="unicode"),
        })

    payload = {
        "chip_id": chip_id,
        "policy_version": policy_version,
        "mpu": mpu_name,
        "hw": hw,
        "regions": regions,
    }

    key = MPU_KEY.format(
        chip_id=chip_id,
        policy_version=policy_version,
        mpu_name=mpu_name.upper(),
    )

    redis_client.set(key, json.dumps(payload))





import re

def _normalize(s: str) -> str:
    return re.sub(r"[^a-z0-9]+", "", s.lower())


def _extract_alias(name: str) -> Optional[str]:
    """
    Examples:
      "SM8850 (KaanapaliT)" -> kaanapali
      "SM8850 (KaanapaliS)" -> kaanapali
      "Zealis v3"           -> zealis
    """
    m = re.search(r"\(([^)]+)\)", name)
    if m:
        base = m.group(1)
        base = re.sub(r"(tsmc|samsung|[ts])$", "", base.lower())
        return _normalize(base)

    # fallback: first token
    return _normalize(name.split()[0])


def _extract_family(name: str) -> Optional[str]:
    # For now: family == alias
    return _extract_alias(name)



from packaging.version import Version
import json

def _resolve_chip_id(
    self,
    chip_name: str,
    version: str | None = None,
) -> str | None:
    """
    Resolve chip name / alias / family → chip_id.
    If version is None → pick latest active.
    """

    name = chip_name.lower().strip()

    # ------------------------------------------------
    # 1. Fast path: direct name/alias index
    # ------------------------------------------------
    chip_id = self.redis.get(
        CHIP_NAME_KEY.format(name=name)
    )
    if chip_id:
        return chip_id.decode() if isinstance(chip_id, bytes) else chip_id

    # ------------------------------------------------
    # 2. Fallback: family-based resolution
    # ------------------------------------------------
    raw = self.redis.get(CHIP_LIST_KEY)
    if not raw:
        return None

    chips = json.loads(raw)

    # Match by family OR name token
    candidates = [
        c for c in chips
        if (
            c.get("family", "").lower() == name
            or name in c.get("name", "").lower()
        )
    ]

    if not candidates:
        return None

    # ------------------------------------------------
    # 3. Version handling
    # ------------------------------------------------
    if version:
        for c in candidates:
            if c.get("version") == version:
                return str(c["id"])
        return None  # explicit version requested but not found

    # ------------------------------------------------
    # 4. No version → pick latest active
    # ------------------------------------------------
    active = [c for c in candidates if c.get("active")]
    pool = active if active else candidates

    best = max(pool, key=lambda c: Version(c["version"]))
    return str(best["id"])



from packaging.version import Version
from typing import Optional, Iterable


def find_chip(
    redis,
    query: str,
    requested_version: Optional[str] = None,
) -> Optional[Chip]:
    """
    Resolve a chip from Redis using:
    1. name / alias
    2. family
    3. version selection
    """

    q = query.lower().strip()

    # -------------------------------------------------
    # 1. Exact match via index (name or alias)
    # -------------------------------------------------
    chip_id = redis.get(CHIP_NAME_KEY.format(name=q))
    if chip_id:
        return _load_chip(redis, chip_id, requested_version)

    # -------------------------------------------------
    # 2. Family-based fallback
    # -------------------------------------------------
    chips = _load_all_chips(redis)

    family_matches = [
        c for c in chips
        if c.family and c.family.lower() == q
    ]

    if not family_matches:
        return None

    return _select_version(family_matches, requested_version)


def _load_chip(redis, chip_id: bytes, requested_version: Optional[str]) -> Chip:
    raw = redis.get(CHIP_ID_KEY.format(chip_id=int(chip_id)))
    chip = Chip.from_api(json.loads(raw))

    if not requested_version or chip.version == requested_version:
        return chip

    raise ValueError(
        f"Requested version {requested_version} not available for {chip.name}"
    )


def _load_all_chips(redis) -> list[Chip]:
    raw = redis.get(CHIP_LIST_KEY)
    if not raw:
        raise RuntimeError("Chip list not ingested")

    items = json.loads(raw)
    return [Chip.from_api(i) for i in items if i]

def _select_version(
    chips: Iterable[Chip],
    requested_version: Optional[str],
) -> Chip:
    valid = [c for c in chips if c.active]

    if not valid:
        raise ValueError("No active chips found")

    if requested_version:
        for c in valid:
            if c.version == requested_version:
                return c
        raise ValueError(f"Version {requested_version} not found")

    # Latest published
    return max(valid, key=lambda c: Version(c.version))





def _extract_family(name: str) -> Optional[str]:
    """
    Normalize chip family.

    Examples:
    - "SM8850 (KaanapalliT)" → "kaanapalli"
    - "SM8850 (KaanapalliS) r2" → "kaanapalli"
    """
    if "(" not in name:
        return None

    inside = name.split("(", 1)[1].split(")", 1)[0]
    base = inside.rstrip("TS0123456789 ").lower()

    return base or None


def _extract_alias(name: str) -> Optional[str]:
    """
    Optional alias for direct lookup.

    Examples:
    - "SM8850 (KaanapalliT)" → "kaanapallit"
    - "SM8850 (KaanapalliS) r2" → "kaanapallis"
    """
    if "(" not in name:
        return None

    inside = name.split("(", 1)[1].split(")", 1)[0]
    return inside.strip().lower() or None




import asyncio
import logging
import redis

from ipcatalog.client import IPCatalogClient
from orchestrator import PolicyOrchestrator

logging.basicConfig(level=logging.INFO)

REDIS_HOST = "localhost"
REDIS_PORT = 6379


async def main():
    # Redis
    redis_client = redis.Redis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        decode_responses=True,
    )

    # IPCatalog API client
    ipcat = IPCatalogClient(
        base_url="https://ipcatalog-api.qualcomm.com/api/1",
    )

    orchestrator = PolicyOrchestrator(
        redis_client=redis_client,
        ipcat_client=ipcat,
    )

    # --- STEP 1: Ensure chip list (TTL driven) ---
    await orchestrator.ensure_chip_list()
    print("✅ Chip list ready")

    # --- STEP 2: Fetch MPU policy ---
    result = await orchestrator.get_policy_by_mpu(
        chip_name="kaanapalli",
        version="v3",
        mpu_name="AOSS_MPU",
    )

    print("\n=== MPU POLICY ===")
    print(result)


if __name__ == "__main__":
    asyncio.run(main())





import asyncio
import json
import logging
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict, List

from ipcatalog.ingest import (
    fetch_chip_xml,
    extract_mpus,
    ingest_single_mpu,   # single MPU → hw + regions
)
from ipcatalog.redis_keys import (
    CHIP_LIST_KEY,
    CHIP_ALIAS_KEY,
    INGEST_LOCK_KEY,
    INGEST_DONE_KEY,
    MPU_KEY,
)

logger = logging.getLogger("orchestrator")

EXECUTOR = ThreadPoolExecutor(max_workers=8)

LOCK_TTL = 60            # seconds
DONE_TTL = 24 * 3600     # 1 day


class PolicyOrchestrator:
    """
    Thin decision-making layer.
    No parsing.
    No Redis schema logic.
    No XML knowledge.
    """

    def __init__(self, redis_client, ipcat_client):
        self.redis = redis_client
        self.client = ipcat_client

    # ==========================================================
    # PUBLIC ENTRY POINT
    # ==========================================================
    async def get_policy_by_mpu(
        self,
        chip_name: str,
        version: str,
        mpu_name: str,
    ) -> Dict[str, Any]:
        """
        Main API entry point.
        """

        chip_name = chip_name.lower()
        mpu_name = mpu_name.upper()

        chip_id = self._resolve_chip_id(chip_name)
        if not chip_id:
            raise ValueError(f"No chip found for chipname: {chip_name}")

        await self._ensure_ingested(chip_name, version)

        key = MPU_KEY.format(
            chip=chip_name,
            version=version,
            mpu=mpu_name,
        )

        raw = self.redis.get(key)
        if not raw:
            raise KeyError(f"MPU not found: {mpu_name}")

        return json.loads(raw)

    # ==========================================================
    # INGESTION ORCHESTRATION
    # ==========================================================
    async def _ensure_ingested(self, chip: str, version: str) -> None:
        """
        Cache-miss / TTL based ingestion trigger.
        """

        done_key = INGEST_DONE_KEY.format(chip=chip, version=version)
        lock_key = INGEST_LOCK_KEY.format(chip=chip, version=version)

        # Fast path
        if self.redis.exists(done_key):
            return

        # Someone else is ingesting
        if not self.redis.set(lock_key, 1, nx=True, ex=LOCK_TTL):
            logger.info("Ingestion already in progress: %s %s", chip, version)
            return

        try:
            logger.info("Starting ingestion: %s %s", chip, version)
            await self._ingest_chip(chip, version)
            self.redis.set(done_key, 1, ex=DONE_TTL)

        finally:
            self.redis.delete(lock_key)

    async def _ingest_chip(self, chip: str, version: str) -> None:
        """
        Fetch XML → list MPUs → ingest each MPU (threaded).
        """

        xml_text = await fetch_chip_xml(self.client, chip, version)
        mpus = extract_mpus(xml_text)

        loop = asyncio.get_running_loop()
        tasks = []

        for mpu in mpus:
            tasks.append(
                loop.run_in_executor(
                    EXECUTOR,
                    ingest_single_mpu,
                    xml_text,
                    chip,
                    version,
                    mpu,
                    self.redis,
                )
            )

        # Fail fast if any MPU fails
        for t in asyncio.as_completed(tasks):
            await t

        logger.info("Ingestion complete: %s %s", chip, version)

    # ==========================================================
    # CHIP RESOLUTION
    # ==========================================================
    def _resolve_chip_id(self, chip_name: str) -> str | None:
        """
        Resolve chip alias → chip_id via Redis.
        """
        key = CHIP_ALIAS_KEY.format(name=chip_name)
        val = self.redis.get(key)
        return val.decode() if isinstance(val, bytes) else val

    # ==========================================================
    # CHIP LIST (TTL-DRIVEN)
    # ==========================================================
    async def ensure_chip_list(self) -> None:
        """
        Refresh chip list if TTL expired.
        """

        if self.redis.exists(CHIP_LIST_KEY):
            return

        logger.info("Refreshing chip list")

        chips = await self.client.list_chips()
        payload = json.dumps(chips)

        self.redis.setex(CHIP_LIST_KEY, 36 * 3600, payload)

        for c in chips:
            for name in filter(None, [c["name"], c.get("alias")]):
                self.redis.setex(
                    CHIP_ALIAS_KEY.format(name=name.lower()),
                    36 * 3600,
                    c["id"],
                )



from concurrent.futures import ThreadPoolExecutor, as_completed
import xml.etree.ElementTree as ET

def ingest_mpus(
    xml_text: str,
    chip: str,
    version: str,
    redis_client,
    max_workers: int = 8,
) -> None:
    root = ET.fromstring(xml_text)
    mpu_elems = root.findall(".//MPU")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(
                ingest_single_mpu,
                mpu_elem,
                chip,
                version,
                redis_client,
            )
            for mpu_elem in mpu_elems
        ]

        # Fail fast if any MPU ingestion fails
        for future in as_completed(futures):
            future.result()

def ingest_single_mpu(
    mpu_elem: ET.Element,
    chip: str,
    version: str,
    redis_client,
) -> None:
    mpu_name = mpu_elem.attrib["name"]

    hw = extract_hw_parameters(mpu_elem)

    redis_key = f"ipcat:mpu:{chip}:{version}:{mpu_name}"

    redis_client.set(
        redis_key,
        json.dumps({
            "mpu": mpu_name,
            "chip": chip,
            "version": version,
            "hw": hw,
        })
    )

# backend/ingestion/xml_parser.py
from typing import Dict
import xml.etree.ElementTree as ET

def extract_hw_parameters(mpu_elem: ET.Element) -> Dict[str, Dict[str, str]]:
    """
    Extract HWDesignParameters and HWIntegrationParameters
    for a single MPU element.
    """

    result = {
        "design": {},
        "integration": {},
    }

    hw_cfg = mpu_elem.find("HWConfig")
    if hw_cfg is None:
        return result

    design = hw_cfg.find("HWDesignParameters")
    if design is not None:
        for p in design.findall("Parameter"):
            name = p.attrib.get("name")
            value = p.attrib.get("value")
            if name and value is not None:
                result["design"][name] = value

    integration = hw_cfg.find("HWIntegrationParameters")
    if integration is not None:
        for p in integration.findall("Parameter"):
            name = p.attrib.get("name")
            value = p.attrib.get("value")
            if name and value is not None:
                result["integration"][name] = value

    return result


import asyncio
from concurrent.futures import ThreadPoolExecutor
import xml.etree.ElementTree as ET
from .models import Policy
from .redis_store import (
    cache_chip, cache_policies,
    bloom_add, bloom_exists
)
from .ipcatalog import IPCatalogClient

EXECUTOR = ThreadPoolExecutor(max_workers=8)
SEMAPHORE = asyncio.Semaphore(4)

def parse_mpu(xml_text, chip, version, mpu):
    root = ET.fromstring(xml_text)
    policies = []
    for rg in root.findall(f".//MPU[@name='{mpu}']/Region"):
        region = int(rg.attrib["id"])
        policies.append(
            Policy(
                chip=chip,
                version=version,
                mpu=mpu,
                region=region,
                raw_text=ET.tostring(rg, encoding="unicode"),
            )
        )
    return policies

async def ingest_chip(chip: str, version: str, client: IPCatalogClient):
    if bloom_exists(chip, version):
        return

    async with aiohttp.ClientSession() as session:
        xml = await client.get(session, f"/xpu/policy/export?chip={chip}&version={version}")
        mpus = extract_mpus(xml)  # simple XPath

        loop = asyncio.get_event_loop()
        all_policies = []

        for mpu in mpus:
            async with SEMAPHORE:
                policies = await loop.run_in_executor(
                    EXECUTOR, parse_mpu, xml, chip, version, mpu
                )
                all_policies.extend(policies)

        cache_policies(all_policies)
        bloom_add(chip, version)






import aiohttp
import time
import json
from pathlib import Path

TOKEN_FILE = Path("/tmp/ipcat.token")

class IPCatalogClient:
    def __init__(self, base_url, username, password):
        self.base = base_url
        self.username = username
        self.password = password
        self.token = None

    async def _load_token(self):
        if TOKEN_FILE.exists():
            self.token = TOKEN_FILE.read_text().strip()

    async def _save_token(self):
        TOKEN_FILE.write_text(self.token)

    async def _fetch_token(self, session):
        async with session.post(
            f"{self.base}/auth/login",
            json={"username": self.username, "password": self.password},
        ) as r:
            data = await r.json()
            self.token = data["token"]
            await self._save_token()

    async def get(self, session, path):
        if not self.token:
            await self._load_token()
        headers = {"Authorization": f"Bearer {self.token}"}
        async with session.get(f"{self.base}{path}", headers=headers) as r:
            if r.status == 401:
                await self._fetch_token(session)
                return await self.get(session, path)
            return await r.json()





import re

def normalize_alias(s: str) -> str:
    s = s.lower()
    s = re.sub(r"\(.*?\)", "", s)   # drop (rolex)
    s = re.sub(r"[^a-z0-9]+", "", s)
    return s





def cache_chips(chips: Iterable[Chip]) -> None:
    pipe = redis_client.pipeline(transaction=True)

    # Store full list
    payload = [c.to_dict() for c in chips]
    pipe.setex(
        CHIP_LIST_KEY,
        TTL_CHIPS,
        json.dumps(payload),
    )

    for chip in chips:
        # Store per-chip object
        pipe.setex(
            CHIP_ID_KEY.format(chip_id=chip.id),
            TTL_CHIPS,
            json.dumps(chip.to_dict()),
        )

        # Index by name + optional alias
        for key in _chip_keys(chip):
            pipe.setex(
                CHIP_ALIAS_KEY.format(chip_name=key),
                TTL_CHIPS,
                chip.id,
            )

    pipe.execute()




from dataclasses import dataclass
from typing import Optional, Dict, Any


@dataclass(frozen=True)
class ChipFamily:
    id: int
    name: str
    generation: Optional[int] = None


@dataclass(frozen=True)
class ChipType:
    id: int
    name: str


@dataclass(frozen=True)
class Chip:
    id: int
    name: str
    family: ChipFamily
    type: ChipType
    baseline_chip: Optional[str] = None
    last_revised: Optional[str] = None

    @staticmethod
    def from_api(item: Dict[str, Any]) -> "Chip":
        """
        Convert raw IPCatalog API JSON → Chip dataclass.
        Raises KeyError on schema drift.
        """
        return Chip(
            id=item["id"],
            name=item["name"],
            family=ChipFamily(
                id=item["family"]["id"],
                name=item["family"]["name"],
                generation=item["family"].get("generation"),
            ),
            type=ChipType(
                id=item["type"]["id"],
                name=item["type"]["name"],
            ),
            baseline_chip=item.get("baseline_chip"),
            last_revised=item.get("last_revised"),
        )

CHIP_LIST_KEY = "ipcat:chip:list"
CHIP_ALIAS_KEY = "ipcat:chip:alias:{name}"
CHIP_SCHEMA_KEY = "ipcat:chip:schema_version"

CHIP_SCHEMA_VERSION = "v2"   # bump when Chip dataclass changes
TTL_CHIPS = 36 * 3600


def ensure_chips(self):
    """
    Ensures chip list exists in Redis.
    Auto-heals Redis if schema version changed.
    """

    cached_schema = redis_client.get(CHIP_SCHEMA_KEY)
    if cached_schema != CHIP_SCHEMA_VERSION:
        logger.warning(
            "Chip schema drift detected (redis=%s, code=%s). Flushing chip cache.",
            cached_schema,
            CHIP_SCHEMA_VERSION,
        )
        self._flush_chip_cache()

    if redis_client.exists(CHIP_LIST_KEY):
        return

    raw = self.client.list_chips_raw()  # raw JSON
    chips = []

    for item in raw:
        try:
            chip = Chip.from_api(item)
            chips.append(chip)
        except KeyError as e:
            logger.error("Skipping invalid chip entry: %s", item)
            continue

    if not chips:
        raise RuntimeError("No valid chips returned from IPCatalog")

    # Store schema version
    redis_client.set(CHIP_SCHEMA_KEY, CHIP_SCHEMA_VERSION)

    # Store chip IDs list
    redis_client.setex(
        CHIP_LIST_KEY,
        TTL_CHIPS,
        ",".join(str(c.id) for c in chips),
    )

    # Store alias → id mapping
    for chip in chips:
        redis_client.setex(
            CHIP_ALIAS_KEY.format(name=chip.name.lower()),
            TTL_CHIPS,
            chip.id,
        )

    logger.info("Cached %d chips in Redis", len(chips))



def _flush_chip_cache(self):
    for key in redis_client.scan_iter("ipcat:chip:*"):
        redis_client.delete(key)


def print_chips(chips: list[Chip]) -> None:
    print("\n=== IPCatalog Chips ===")
    for c in chips:
        print(
            f"- {c.name} "
            f"(id={c.id}, family={c.family.name}, type={c.type.name})"
        )


from typing import List, Dict, Any

Chip = Dict[str, Any]

def list_chips(self) -> List[Chip]:
    data = self.request("GET", "/chip")

    if not isinstance(data, list):
        raise TypeError(f"Expected list from /chip, got {type(data)}")

    valid_chips: List[Chip] = []

    for i, item in enumerate(data):
        if not isinstance(item, dict):
            raise TypeError(f"Item {i} is not a dict: {item!r}")

        # Required fields per IPCatalog API
        required_keys = {"id", "name", "family", "type"}
        missing = required_keys - item.keys()

        if missing:
            raise KeyError(
                f"Chip item {i} missing keys {missing}. Full item={item!r}"
            )

        # Validate nested structures
        if not isinstance(item["family"], dict) or "name" not in item["family"]:
            raise KeyError(f"Chip item {i} has invalid family field: {item['family']!r}")

        if not isinstance(item["type"], dict) or "name" not in item["type"]:
            raise KeyError(f"Chip item {i} has invalid type field: {item['type']!r}")

        valid_chips.append(item)

    return valid_chips





from typing import Generator, List
import time
import math
import xml.etree.ElementTree as ET


def fetch_parsed_policy_export(
    self,
    export_id: int | str,
    export_token: str,
    *,
    page_size: int = 500,
    poll_interval: int = 5,
    timeout: int = 300,
) -> Generator[List[ET.Element], None, None]:
    """
    Polls an async xPU policy export, parses XML, and yields paginated XML elements.

    Yields:
        List[ET.Element]: A page of repeating XML elements (policies / regions).

    Raises:
        TimeoutError: If export generation exceeds timeout.
        RuntimeError: If response is invalid or empty.
    """

    url = join_url(self.base_url, f"xpu/policy/export/{export_id}/")
    start = time.time()

    # ---- Poll until XML is ready ----
    while True:
        resp = self.request(
            "GET",
            api=url,
            params={"raw": True, "token": export_token},
            response="raw",
        )

        status = resp.status_code
        ctype = resp.headers.get("content-type", "").lower()

        logger.info(f"Export poll status={status}, content-type={ctype}")

        if status == 202:
            if time.time() - start > timeout:
                raise TimeoutError("Policy export generation timed out")
            time.sleep(poll_interval)
            continue

        if status != 200:
            raise RuntimeError(f"Unexpected export status {status}")

        if "xml" not in ctype:
            # still preparing file
            time.sleep(poll_interval)
            continue

        break  # XML ready

    # ---- Parse XML ----
    try:
        root = ET.fromstring(resp.content)
    except ET.ParseError as e:
        raise RuntimeError("Failed to parse policy XML") from e

    tag, elements = finding_repeating_elements(root)

    if not elements:
        logger.info("Single XML document detected (no repeating elements)")
        yield [root]
        return

    total = len(elements)
    max_pages = math.ceil(total / page_size)

    logger.info(
        "Parsed policy XML: tag=%s total_elements=%d pages=%d",
        tag,
        total,
        max_pages,
    )

    # ---- Yield paginated results ----
    for page in range(max_pages):
        start_idx = page * page_size
        end_idx = min(start_idx + page_size, total)
        yield elements[start_idx:end_idx]







def select_xpu_policy(
    policies: List[XPUPolicy],
    requested_version: Optional[str] = None
) -> Optional[XPUPolicy]:
    valid = [
        p for p in policies
        if p["published"] and not p["is_deleted"]
    ]

    if not valid:
        return None

    if requested_version:
        for p in valid:
            if p["version"] == requested_version:
                return p
        return None

    # latest published
    return max(valid, key=lambda p: Version(p["version"]))


class IPCatalogClient:
    def __init__(self, base_url, token_manager: TokenManager):
        self.base_url = base_url.rstrip("/")
        self.token_manager = token_manager

    def request(self, method: str, api: str, **kwargs):
        token = self.token_manager.get_token()

        headers = kwargs.pop("headers", {})
        headers["Authorization"] = f"Bearer {token}"

        url = f"{self.base_url}{api}"

        resp = requests.request(
            method,
            url,
            headers=headers,
            timeout=15,
            **kwargs
        )

        if resp.status_code in (401, 403):
            # Token invalid → refresh once
            self.token_manager.invalidate()
            token = self.token_manager.get_token()
            headers["Authorization"] = f"Bearer {token}"

            resp = requests.request(
                method,
                url,
                headers=headers,
                timeout=15,
                **kwargs
            )

        resp.raise_for_status()
        return resp.json()







"""
Central place for Redis key schema.
Keeps keys consistent and debuggable.
"""

CHIP_LIST_KEY = "ipcat:chips:list"
CHIP_ALIAS_KEY = "ipcat:chip:alias:{chip_name}"

POLICY_INDEX_KEY = "ipcat:xpu:policies:{chip_id}:{version}"
POLICY_XML_KEY = "ipcat:xpu:policy:xml:{chip_id}:{policy_id}"

MPU_KEY = "ipcat:mpu:{chip_id}:{policy_id}:{mpu_name}"




import redis
from redisbloom.client import Client as BloomClient

REDIS_HOST = "redis"
REDIS_PORT = 6379

redis_client = redis.Redis(
    host=REDIS_HOST,
    port=REDIS_PORT,
    decode_responses=True
)

# Bloom filter for (chip, version)
bloom = BloomClient(
    host=REDIS_HOST,
    port=REDIS_PORT
)

BLOOM_KEY = "ipcat:bloom:chip_version"

def init_bloom():
    """
    Initialize Bloom filter once.
    """
    try:
        bloom.bfCreate(BLOOM_KEY, 0.01, 100_000)
    except Exception:
        # Already exists
        pass



requests>=2.31.0
redis>=5.0.1
lxml>=4.9.3
tenacity>=8.2.3
python-dotenv>=1.0.0




import io
import json
import time
import zipfile
from itertools import islice


def fetch_policy_export_paginated(
    client,
    export_id,
    export_token,
    page_size=500,
    poll_interval=5,
    timeout=300,
):
    """
    Yields paginated policy records from a large async export.

    Yields:
        list[dict]  # page_size chunk
    """

    # ---- 1. Wait until export is ready ----
    start = time.time()

    while True:
        resp = client.request(
            "GET",
            api=f"xpu/policy/export/{export_id}/",
            params={"raw": True, "token": export_token},
            response="raw",
        )

        if resp.status_code == 202:
            if time.time() - start > timeout:
                raise TimeoutError("Export generation timed out")
            time.sleep(poll_interval)
            continue

        if resp.status_code != 200:
            raise RuntimeError(
                f"Export failed: {resp.status_code} - {resp.content[:200]}"
            )

        break  # ready

    # ---- 2. Load content into memory ----
    content = resp.content
    content_type = resp.headers.get("content-type", "").lower()

    # ---- 3. Extract JSON records ----
    records = None

    # Case A: ZIP export
    if "application/zip" in content_type or content.startswith(b"PK\x03\x04"):
        with zipfile.ZipFile(io.BytesIO(content)) as z:
            for name in z.namelist():
                if name.endswith(".json"):
                    with z.open(name) as f:
                        data = json.load(f)

                        # normalize structure
                        if isinstance(data, list):
                            records = data
                        elif isinstance(data, dict):
                            records = (
                                data.get("policies")
                                or data.get("items")
                                or data.get("data")
                            )

                        if records:
                            break

        if records is None:
            raise RuntimeError("No JSON records found in export ZIP")

    # Case B: Direct JSON
    else:
        data = json.loads(content.decode())
        if isinstance(data, list):
            records = data
        elif isinstance(data, dict):
            records = (
                data.get("policies")
                or data.get("items")
                or data.get("data")
            )

    if not records:
        raise RuntimeError("No paginatable records found")

    # ---- 4. Paginate (generator) ----
    it = iter(records)

    while True:
        page = list(islice(it, page_size))
        if not page:
            break
        yield page





import io
import json
import zipfile


def fetch_parsed_policy_export(client, export_id, export_token):
    """
    Downloads an async policy export into memory and returns parsed policy data.

    Returns:
        dict | list | str
            Parsed policy data (JSON) or raw text if non-JSON.
    Raises:
        RuntimeError on unexpected response or empty export.
    """

    resp = client.request(
        "GET",
        api=f"xpu/policy/export/{export_id}/",
        params={
            "raw": True,
            "token": export_token,
        },
        response="raw",
    )

    if resp.status_code != 200:
        raise RuntimeError(
            f"Export download failed: {resp.status_code} - {resp.content[:200]}"
        )

    content_type = resp.headers.get("content-type", "").lower()
    content = resp.content

    # ---- Case 1: JSON directly returned ----
    if "application/json" in content_type:
        return json.loads(content.decode("utf-8"))

    # ---- Case 2: ZIP archive ----
    if "application/zip" in content_type or content.startswith(b"PK\x03\x04"):
        zip_bytes = io.BytesIO(content)

        parsed = {}

        with zipfile.ZipFile(zip_bytes) as z:
            if not z.namelist():
                raise RuntimeError("Export ZIP is empty")

            for name in z.namelist():
                with z.open(name) as f:
                    if name.endswith(".json"):
                        parsed[name] = json.load(f)
                    else:
                        parsed[name] = f.read().decode(errors="ignore")

        return parsed

    # ---- Case 3: Fallback (text / unknown) ----
    try:
        return content.decode("utf-8")
    except UnicodeDecodeError:
        return content








curl -v \
  -X POST \
  https://ipcatalog-api.qualcomm.com/api/1/auth/token/login/ \
  -H "Content-Type: application/x-www-form-urlencoded" \
  --data "username=YOUR_USERNAME&password=YOUR_PASSWORD"


# rag/executors/sql_helpers.py

def regions_by_project(project: str, mpu_name: str | None = None):
    base = """
        SELECT DISTINCT region
        FROM mpu_regions
        WHERE project = :project
    """
    params = {"project": project}

    if mpu_name:
        base += " AND mpu_name = :mpu_name"
        params["mpu_name"] = mpu_name

    base += " ORDER BY region"
    return base, params

def region_count_by_project(project: str, mpu_name: str | None = None):
    base = """
        SELECT COUNT(DISTINCT region) AS count
        FROM mpu_regions
        WHERE project = :project
    """
    params = {"project": project}

    if mpu_name:
        base += " AND mpu_name = :mpu_name"
        params["mpu_name"] = mpu_name

    return base, params

# rag/executors/region_list.py

from rag.executors.base import BaseExecutor
from rag.query_helpers.models import QueryFacts
from rag.executors.sql_helpers import regions_by_project


class RegionListExecutor(BaseExecutor):
    name = "region_list"

    def execute(self, facts: QueryFacts):
        project = facts.project
        mpu_name = facts.mpu_name

        if not project:
            return self.fail("project is required")

        sql, params = regions_by_project(project, mpu_name)
        rows = self.run_query(sql, params)

        if not rows:
            return {
                "mode": "TEXT",
                "text": None,
                "rows": [],
                "confidence": 0.0,
                "explainability": "No regions found for the given project/MPU.",
                "sources": ["mpu_regions"],
            }

        regions = [r["region"] for r in rows]

        return {
            "mode": "LIST",
            "text": None,
            "rows": regions,
            "confidence": 1.0,
            "explainability": f"Regions supported in project {project}"
            + (f" for MPU {mpu_name}" if mpu_name else ""),
            "sources": ["mpu_regions"],
        }
# rag/executors/region_count.py

from rag.executors.base import BaseExecutor
from rag.query_helpers.models import QueryFacts
from rag.executors.sql_helpers import region_count_by_project


class RegionCountExecutor(BaseExecutor):
    name = "region_count"

    def execute(self, facts: QueryFacts):
        project = facts.project
        mpu_name = facts.mpu_name

        if not project:
            return self.fail("project is required")

        sql, params = region_count_by_project(project, mpu_name)
        rows = self.run_query(sql, params)

        count = rows[0]["count"] if rows else 0

        return {
            "mode": "TEXT",
            "text": str(count),
            "rows": None,
            "confidence": 1.0,
            "explainability": f"Number of regions in project {project}"
            + (f" for MPU {mpu_name}" if mpu_name else ""),
            "sources": ["mpu_regions"],
        }







REGION EXTRACTION RULES (HIGHEST PRIORITY):

- If the user mentions the word "region" followed by a number:
  - Extract that number as an integer
  - Set region = <number>
  - Set entity = REGION
  - Set operation = LOOKUP
  - DO NOT populate addr_start or addr_end

- Numeric values associated with "region" MUST NEVER be treated as addresses.

- If both a region number and an address are present:
  - Region number maps to region
  - Hexadecimal value maps to address

ADDRESS EXTRACTION GUARDRAILS:

- Treat a value as an ADDRESS ONLY IF:
  - It starts with "0x", OR
  - The word "address" explicitly appears

- Decimal numbers MUST NOT be treated as addresses unless explicitly labeled.






from enum import Enum
from typing import Optional
from pydantic import BaseModel, Field

class Intent(str, Enum):
    POLICY_LOOKUP = "POLICY_LOOKUP"
    POLICY_LIST = "POLICY_LIST"
    CATALOG_LIST = "CATALOG_LIST"
    EXPLAIN_TEXT = "EXPLAIN_TEXT"
    UNKNOWN = "UNKNOWN"

class Operation(str, Enum):
    LOOKUP = "LOOKUP"
    LIST = "LIST"
    COUNT = "COUNT"
    COMPARE = "COMPARE"
    EXPLAIN = "EXPLAIN"
    UNKNOWN = "UNKNOWN"


class Entity(str, Enum):
    POLICY = "POLICY"
    PROJECT = "PROJECT"
    VERSION = "VERSION"
    REGION = "REGION"
    ADDRESS = "ADDRESS"
    UNKNOWN = "UNKNOWN"

class QueryFacts(BaseModel):
    """
    QueryFacts is a PURE extraction object.

    It contains ONLY facts explicitly present in the user query.
    No validation, inference, or enforcement happens here.
    """

    # --- classification ---
    intent: Intent = Field(
        default=Intent.UNKNOWN,
        description="Why the system is invoked"
    )

    operation: Operation = Field(
        default=Operation.UNKNOWN,
        description="How the user phrased the request"
    )

    entity: Entity = Field(
        default=Entity.UNKNOWN,
        description="Primary domain object referenced"
    )

    # --- selectors / filters ---
    project: Optional[str] = Field(
        default=None,
        description="Project name if explicitly mentioned"
    )

    region: Optional[str] = Field(
        default=None,
        description="Region name if explicitly mentioned"
    )

    version: Optional[str] = Field(
        default=None,
        description="Version identifier if explicitly mentioned"
    )

    # --- address semantics ---
    addr_start: Optional[int] = Field(
        default=None,
        description="Start address in decimal (single address or range start)"
    )

    addr_end: Optional[int] = Field(
        default=None,
        description="End address in decimal (only if range explicitly provided)"
    )

    # --- raw provenance ---
    raw_query: Optional[str] = Field(
        default=None,
        description="Original user query (verbatim)"
    )










You are a STRICT but NON-JUDGMENTAL structured facts extraction engine.

Your ONLY responsibility is to extract explicit, surface-level facts
from the user query and return a JSON object that matches the QueryFacts schema.

You MUST follow these principles:

1. Extraction only
   - Do NOT answer the question
   - Do NOT explain anything
   - Do NOT validate correctness
   - Do NOT apply business rules
   - Do NOT reason about executability

2. Monotonicity
   - If a value is explicitly present, extract it
   - If a value is not clearly present, return null
   - Never invent, infer, or guess missing data

3. Schema obedience
   - Output ONLY valid JSON
   - Match the QueryFacts schema EXACTLY
   - All enum values must come from the enum definitions
   - All fields must be present (use null if unknown)

4. Partial facts are VALID
   - It is acceptable to return incomplete facts
   - Downstream systems will decide if the facts are sufficient

5. No cross-field enforcement
   - Do NOT enforce relationships between fields
   - Do NOT resolve conflicts
   - Do NOT reject combinations

Your output must ALWAYS be a valid QueryFacts JSON object.
Returning null values is ALWAYS allowed.

INTENT (what the user is asking for):
- POLICY_LOOKUP     → asking about a specific policy
- CATALOG_LIST      → listing projects, versions, items
- POLICY_LIST       → listing multiple policies
- EXPLAIN_TEXT      → conceptual explanation (no lookup)
- UNKNOWN            → intent cannot be determined

OPERATION (how the user phrases the request):
- LOOKUP
- LIST
- COUNT
- COMPARE
- EXPLAIN
- UNKNOWN

ENTITY (what object is referenced, if any):
- POLICY
- PROJECT
- VERSION
- REGION
- ADDRESS
- UNKNOWN

Intent extraction:
- Choose the intent that best matches the wording
- If unclear, use UNKNOWN

Operation extraction:
- Use LOOKUP if the query asks for details of one thing
- Use LIST if multiple items are requested
- Otherwise use UNKNOWN

Entity extraction:
- Extract only entities explicitly mentioned
- If multiple entities appear, choose the most central one
- If unclear, use UNKNOWN

Address handling rules:

- If a hexadecimal address like 0xEF01000 is present:
  - Convert it to a decimal integer
  - Set addr_start to that integer
  - Set addr_end to null

- If an address range is present (e.g. 0x1000–0x1FFF):
  - Convert both ends to decimal integers
  - Set addr_start and addr_end accordingly

- If no address is present:
  - addr_start = null
  - addr_end = null

Never fabricate addr_end.
Never expand a single address into a range.

- Extract project name only if explicitly stated
- Extract region only if explicitly stated
- Extract version only if explicitly stated
- Otherwise return null

Return ONLY a valid JSON object that conforms to the QueryFacts schema.
Do not include commentary, markdown, or explanations.
