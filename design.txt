
from dataclasses import dataclass
from typing import Optional, Dict, Any


@dataclass(frozen=True)
class ChipFamily:
    id: int
    name: str
    generation: Optional[int] = None


@dataclass(frozen=True)
class ChipType:
    id: int
    name: str


@dataclass(frozen=True)
class Chip:
    id: int
    name: str
    family: ChipFamily
    type: ChipType
    baseline_chip: Optional[str] = None
    last_revised: Optional[str] = None

    @staticmethod
    def from_api(item: Dict[str, Any]) -> "Chip":
        """
        Convert raw IPCatalog API JSON → Chip dataclass.
        Raises KeyError on schema drift.
        """
        return Chip(
            id=item["id"],
            name=item["name"],
            family=ChipFamily(
                id=item["family"]["id"],
                name=item["family"]["name"],
                generation=item["family"].get("generation"),
            ),
            type=ChipType(
                id=item["type"]["id"],
                name=item["type"]["name"],
            ),
            baseline_chip=item.get("baseline_chip"),
            last_revised=item.get("last_revised"),
        )

CHIP_LIST_KEY = "ipcat:chip:list"
CHIP_ALIAS_KEY = "ipcat:chip:alias:{name}"
CHIP_SCHEMA_KEY = "ipcat:chip:schema_version"

CHIP_SCHEMA_VERSION = "v2"   # bump when Chip dataclass changes
TTL_CHIPS = 36 * 3600


def ensure_chips(self):
    """
    Ensures chip list exists in Redis.
    Auto-heals Redis if schema version changed.
    """

    cached_schema = redis_client.get(CHIP_SCHEMA_KEY)
    if cached_schema != CHIP_SCHEMA_VERSION:
        logger.warning(
            "Chip schema drift detected (redis=%s, code=%s). Flushing chip cache.",
            cached_schema,
            CHIP_SCHEMA_VERSION,
        )
        self._flush_chip_cache()

    if redis_client.exists(CHIP_LIST_KEY):
        return

    raw = self.client.list_chips_raw()  # raw JSON
    chips = []

    for item in raw:
        try:
            chip = Chip.from_api(item)
            chips.append(chip)
        except KeyError as e:
            logger.error("Skipping invalid chip entry: %s", item)
            continue

    if not chips:
        raise RuntimeError("No valid chips returned from IPCatalog")

    # Store schema version
    redis_client.set(CHIP_SCHEMA_KEY, CHIP_SCHEMA_VERSION)

    # Store chip IDs list
    redis_client.setex(
        CHIP_LIST_KEY,
        TTL_CHIPS,
        ",".join(str(c.id) for c in chips),
    )

    # Store alias → id mapping
    for chip in chips:
        redis_client.setex(
            CHIP_ALIAS_KEY.format(name=chip.name.lower()),
            TTL_CHIPS,
            chip.id,
        )

    logger.info("Cached %d chips in Redis", len(chips))



def _flush_chip_cache(self):
    for key in redis_client.scan_iter("ipcat:chip:*"):
        redis_client.delete(key)


def print_chips(chips: list[Chip]) -> None:
    print("\n=== IPCatalog Chips ===")
    for c in chips:
        print(
            f"- {c.name} "
            f"(id={c.id}, family={c.family.name}, type={c.type.name})"
        )


from typing import List, Dict, Any

Chip = Dict[str, Any]

def list_chips(self) -> List[Chip]:
    data = self.request("GET", "/chip")

    if not isinstance(data, list):
        raise TypeError(f"Expected list from /chip, got {type(data)}")

    valid_chips: List[Chip] = []

    for i, item in enumerate(data):
        if not isinstance(item, dict):
            raise TypeError(f"Item {i} is not a dict: {item!r}")

        # Required fields per IPCatalog API
        required_keys = {"id", "name", "family", "type"}
        missing = required_keys - item.keys()

        if missing:
            raise KeyError(
                f"Chip item {i} missing keys {missing}. Full item={item!r}"
            )

        # Validate nested structures
        if not isinstance(item["family"], dict) or "name" not in item["family"]:
            raise KeyError(f"Chip item {i} has invalid family field: {item['family']!r}")

        if not isinstance(item["type"], dict) or "name" not in item["type"]:
            raise KeyError(f"Chip item {i} has invalid type field: {item['type']!r}")

        valid_chips.append(item)

    return valid_chips





from typing import Generator, List
import time
import math
import xml.etree.ElementTree as ET


def fetch_parsed_policy_export(
    self,
    export_id: int | str,
    export_token: str,
    *,
    page_size: int = 500,
    poll_interval: int = 5,
    timeout: int = 300,
) -> Generator[List[ET.Element], None, None]:
    """
    Polls an async xPU policy export, parses XML, and yields paginated XML elements.

    Yields:
        List[ET.Element]: A page of repeating XML elements (policies / regions).

    Raises:
        TimeoutError: If export generation exceeds timeout.
        RuntimeError: If response is invalid or empty.
    """

    url = join_url(self.base_url, f"xpu/policy/export/{export_id}/")
    start = time.time()

    # ---- Poll until XML is ready ----
    while True:
        resp = self.request(
            "GET",
            api=url,
            params={"raw": True, "token": export_token},
            response="raw",
        )

        status = resp.status_code
        ctype = resp.headers.get("content-type", "").lower()

        logger.info(f"Export poll status={status}, content-type={ctype}")

        if status == 202:
            if time.time() - start > timeout:
                raise TimeoutError("Policy export generation timed out")
            time.sleep(poll_interval)
            continue

        if status != 200:
            raise RuntimeError(f"Unexpected export status {status}")

        if "xml" not in ctype:
            # still preparing file
            time.sleep(poll_interval)
            continue

        break  # XML ready

    # ---- Parse XML ----
    try:
        root = ET.fromstring(resp.content)
    except ET.ParseError as e:
        raise RuntimeError("Failed to parse policy XML") from e

    tag, elements = finding_repeating_elements(root)

    if not elements:
        logger.info("Single XML document detected (no repeating elements)")
        yield [root]
        return

    total = len(elements)
    max_pages = math.ceil(total / page_size)

    logger.info(
        "Parsed policy XML: tag=%s total_elements=%d pages=%d",
        tag,
        total,
        max_pages,
    )

    # ---- Yield paginated results ----
    for page in range(max_pages):
        start_idx = page * page_size
        end_idx = min(start_idx + page_size, total)
        yield elements[start_idx:end_idx]







def select_xpu_policy(
    policies: List[XPUPolicy],
    requested_version: Optional[str] = None
) -> Optional[XPUPolicy]:
    valid = [
        p for p in policies
        if p["published"] and not p["is_deleted"]
    ]

    if not valid:
        return None

    if requested_version:
        for p in valid:
            if p["version"] == requested_version:
                return p
        return None

    # latest published
    return max(valid, key=lambda p: Version(p["version"]))


class IPCatalogClient:
    def __init__(self, base_url, token_manager: TokenManager):
        self.base_url = base_url.rstrip("/")
        self.token_manager = token_manager

    def request(self, method: str, api: str, **kwargs):
        token = self.token_manager.get_token()

        headers = kwargs.pop("headers", {})
        headers["Authorization"] = f"Bearer {token}"

        url = f"{self.base_url}{api}"

        resp = requests.request(
            method,
            url,
            headers=headers,
            timeout=15,
            **kwargs
        )

        if resp.status_code in (401, 403):
            # Token invalid → refresh once
            self.token_manager.invalidate()
            token = self.token_manager.get_token()
            headers["Authorization"] = f"Bearer {token}"

            resp = requests.request(
                method,
                url,
                headers=headers,
                timeout=15,
                **kwargs
            )

        resp.raise_for_status()
        return resp.json()







"""
Central place for Redis key schema.
Keeps keys consistent and debuggable.
"""

CHIP_LIST_KEY = "ipcat:chips:list"
CHIP_ALIAS_KEY = "ipcat:chip:alias:{chip_name}"

POLICY_INDEX_KEY = "ipcat:xpu:policies:{chip_id}:{version}"
POLICY_XML_KEY = "ipcat:xpu:policy:xml:{chip_id}:{policy_id}"

MPU_KEY = "ipcat:mpu:{chip_id}:{policy_id}:{mpu_name}"




import redis
from redisbloom.client import Client as BloomClient

REDIS_HOST = "redis"
REDIS_PORT = 6379

redis_client = redis.Redis(
    host=REDIS_HOST,
    port=REDIS_PORT,
    decode_responses=True
)

# Bloom filter for (chip, version)
bloom = BloomClient(
    host=REDIS_HOST,
    port=REDIS_PORT
)

BLOOM_KEY = "ipcat:bloom:chip_version"

def init_bloom():
    """
    Initialize Bloom filter once.
    """
    try:
        bloom.bfCreate(BLOOM_KEY, 0.01, 100_000)
    except Exception:
        # Already exists
        pass



requests>=2.31.0
redis>=5.0.1
lxml>=4.9.3
tenacity>=8.2.3
python-dotenv>=1.0.0




import io
import json
import time
import zipfile
from itertools import islice


def fetch_policy_export_paginated(
    client,
    export_id,
    export_token,
    page_size=500,
    poll_interval=5,
    timeout=300,
):
    """
    Yields paginated policy records from a large async export.

    Yields:
        list[dict]  # page_size chunk
    """

    # ---- 1. Wait until export is ready ----
    start = time.time()

    while True:
        resp = client.request(
            "GET",
            api=f"xpu/policy/export/{export_id}/",
            params={"raw": True, "token": export_token},
            response="raw",
        )

        if resp.status_code == 202:
            if time.time() - start > timeout:
                raise TimeoutError("Export generation timed out")
            time.sleep(poll_interval)
            continue

        if resp.status_code != 200:
            raise RuntimeError(
                f"Export failed: {resp.status_code} - {resp.content[:200]}"
            )

        break  # ready

    # ---- 2. Load content into memory ----
    content = resp.content
    content_type = resp.headers.get("content-type", "").lower()

    # ---- 3. Extract JSON records ----
    records = None

    # Case A: ZIP export
    if "application/zip" in content_type or content.startswith(b"PK\x03\x04"):
        with zipfile.ZipFile(io.BytesIO(content)) as z:
            for name in z.namelist():
                if name.endswith(".json"):
                    with z.open(name) as f:
                        data = json.load(f)

                        # normalize structure
                        if isinstance(data, list):
                            records = data
                        elif isinstance(data, dict):
                            records = (
                                data.get("policies")
                                or data.get("items")
                                or data.get("data")
                            )

                        if records:
                            break

        if records is None:
            raise RuntimeError("No JSON records found in export ZIP")

    # Case B: Direct JSON
    else:
        data = json.loads(content.decode())
        if isinstance(data, list):
            records = data
        elif isinstance(data, dict):
            records = (
                data.get("policies")
                or data.get("items")
                or data.get("data")
            )

    if not records:
        raise RuntimeError("No paginatable records found")

    # ---- 4. Paginate (generator) ----
    it = iter(records)

    while True:
        page = list(islice(it, page_size))
        if not page:
            break
        yield page





import io
import json
import zipfile


def fetch_parsed_policy_export(client, export_id, export_token):
    """
    Downloads an async policy export into memory and returns parsed policy data.

    Returns:
        dict | list | str
            Parsed policy data (JSON) or raw text if non-JSON.
    Raises:
        RuntimeError on unexpected response or empty export.
    """

    resp = client.request(
        "GET",
        api=f"xpu/policy/export/{export_id}/",
        params={
            "raw": True,
            "token": export_token,
        },
        response="raw",
    )

    if resp.status_code != 200:
        raise RuntimeError(
            f"Export download failed: {resp.status_code} - {resp.content[:200]}"
        )

    content_type = resp.headers.get("content-type", "").lower()
    content = resp.content

    # ---- Case 1: JSON directly returned ----
    if "application/json" in content_type:
        return json.loads(content.decode("utf-8"))

    # ---- Case 2: ZIP archive ----
    if "application/zip" in content_type or content.startswith(b"PK\x03\x04"):
        zip_bytes = io.BytesIO(content)

        parsed = {}

        with zipfile.ZipFile(zip_bytes) as z:
            if not z.namelist():
                raise RuntimeError("Export ZIP is empty")

            for name in z.namelist():
                with z.open(name) as f:
                    if name.endswith(".json"):
                        parsed[name] = json.load(f)
                    else:
                        parsed[name] = f.read().decode(errors="ignore")

        return parsed

    # ---- Case 3: Fallback (text / unknown) ----
    try:
        return content.decode("utf-8")
    except UnicodeDecodeError:
        return content








curl -v \
  -X POST \
  https://ipcatalog-api.qualcomm.com/api/1/auth/token/login/ \
  -H "Content-Type: application/x-www-form-urlencoded" \
  --data "username=YOUR_USERNAME&password=YOUR_PASSWORD"


# rag/executors/sql_helpers.py

def regions_by_project(project: str, mpu_name: str | None = None):
    base = """
        SELECT DISTINCT region
        FROM mpu_regions
        WHERE project = :project
    """
    params = {"project": project}

    if mpu_name:
        base += " AND mpu_name = :mpu_name"
        params["mpu_name"] = mpu_name

    base += " ORDER BY region"
    return base, params

def region_count_by_project(project: str, mpu_name: str | None = None):
    base = """
        SELECT COUNT(DISTINCT region) AS count
        FROM mpu_regions
        WHERE project = :project
    """
    params = {"project": project}

    if mpu_name:
        base += " AND mpu_name = :mpu_name"
        params["mpu_name"] = mpu_name

    return base, params

# rag/executors/region_list.py

from rag.executors.base import BaseExecutor
from rag.query_helpers.models import QueryFacts
from rag.executors.sql_helpers import regions_by_project


class RegionListExecutor(BaseExecutor):
    name = "region_list"

    def execute(self, facts: QueryFacts):
        project = facts.project
        mpu_name = facts.mpu_name

        if not project:
            return self.fail("project is required")

        sql, params = regions_by_project(project, mpu_name)
        rows = self.run_query(sql, params)

        if not rows:
            return {
                "mode": "TEXT",
                "text": None,
                "rows": [],
                "confidence": 0.0,
                "explainability": "No regions found for the given project/MPU.",
                "sources": ["mpu_regions"],
            }

        regions = [r["region"] for r in rows]

        return {
            "mode": "LIST",
            "text": None,
            "rows": regions,
            "confidence": 1.0,
            "explainability": f"Regions supported in project {project}"
            + (f" for MPU {mpu_name}" if mpu_name else ""),
            "sources": ["mpu_regions"],
        }
# rag/executors/region_count.py

from rag.executors.base import BaseExecutor
from rag.query_helpers.models import QueryFacts
from rag.executors.sql_helpers import region_count_by_project


class RegionCountExecutor(BaseExecutor):
    name = "region_count"

    def execute(self, facts: QueryFacts):
        project = facts.project
        mpu_name = facts.mpu_name

        if not project:
            return self.fail("project is required")

        sql, params = region_count_by_project(project, mpu_name)
        rows = self.run_query(sql, params)

        count = rows[0]["count"] if rows else 0

        return {
            "mode": "TEXT",
            "text": str(count),
            "rows": None,
            "confidence": 1.0,
            "explainability": f"Number of regions in project {project}"
            + (f" for MPU {mpu_name}" if mpu_name else ""),
            "sources": ["mpu_regions"],
        }







REGION EXTRACTION RULES (HIGHEST PRIORITY):

- If the user mentions the word "region" followed by a number:
  - Extract that number as an integer
  - Set region = <number>
  - Set entity = REGION
  - Set operation = LOOKUP
  - DO NOT populate addr_start or addr_end

- Numeric values associated with "region" MUST NEVER be treated as addresses.

- If both a region number and an address are present:
  - Region number maps to region
  - Hexadecimal value maps to address

ADDRESS EXTRACTION GUARDRAILS:

- Treat a value as an ADDRESS ONLY IF:
  - It starts with "0x", OR
  - The word "address" explicitly appears

- Decimal numbers MUST NOT be treated as addresses unless explicitly labeled.






from enum import Enum
from typing import Optional
from pydantic import BaseModel, Field

class Intent(str, Enum):
    POLICY_LOOKUP = "POLICY_LOOKUP"
    POLICY_LIST = "POLICY_LIST"
    CATALOG_LIST = "CATALOG_LIST"
    EXPLAIN_TEXT = "EXPLAIN_TEXT"
    UNKNOWN = "UNKNOWN"

class Operation(str, Enum):
    LOOKUP = "LOOKUP"
    LIST = "LIST"
    COUNT = "COUNT"
    COMPARE = "COMPARE"
    EXPLAIN = "EXPLAIN"
    UNKNOWN = "UNKNOWN"


class Entity(str, Enum):
    POLICY = "POLICY"
    PROJECT = "PROJECT"
    VERSION = "VERSION"
    REGION = "REGION"
    ADDRESS = "ADDRESS"
    UNKNOWN = "UNKNOWN"

class QueryFacts(BaseModel):
    """
    QueryFacts is a PURE extraction object.

    It contains ONLY facts explicitly present in the user query.
    No validation, inference, or enforcement happens here.
    """

    # --- classification ---
    intent: Intent = Field(
        default=Intent.UNKNOWN,
        description="Why the system is invoked"
    )

    operation: Operation = Field(
        default=Operation.UNKNOWN,
        description="How the user phrased the request"
    )

    entity: Entity = Field(
        default=Entity.UNKNOWN,
        description="Primary domain object referenced"
    )

    # --- selectors / filters ---
    project: Optional[str] = Field(
        default=None,
        description="Project name if explicitly mentioned"
    )

    region: Optional[str] = Field(
        default=None,
        description="Region name if explicitly mentioned"
    )

    version: Optional[str] = Field(
        default=None,
        description="Version identifier if explicitly mentioned"
    )

    # --- address semantics ---
    addr_start: Optional[int] = Field(
        default=None,
        description="Start address in decimal (single address or range start)"
    )

    addr_end: Optional[int] = Field(
        default=None,
        description="End address in decimal (only if range explicitly provided)"
    )

    # --- raw provenance ---
    raw_query: Optional[str] = Field(
        default=None,
        description="Original user query (verbatim)"
    )










You are a STRICT but NON-JUDGMENTAL structured facts extraction engine.

Your ONLY responsibility is to extract explicit, surface-level facts
from the user query and return a JSON object that matches the QueryFacts schema.

You MUST follow these principles:

1. Extraction only
   - Do NOT answer the question
   - Do NOT explain anything
   - Do NOT validate correctness
   - Do NOT apply business rules
   - Do NOT reason about executability

2. Monotonicity
   - If a value is explicitly present, extract it
   - If a value is not clearly present, return null
   - Never invent, infer, or guess missing data

3. Schema obedience
   - Output ONLY valid JSON
   - Match the QueryFacts schema EXACTLY
   - All enum values must come from the enum definitions
   - All fields must be present (use null if unknown)

4. Partial facts are VALID
   - It is acceptable to return incomplete facts
   - Downstream systems will decide if the facts are sufficient

5. No cross-field enforcement
   - Do NOT enforce relationships between fields
   - Do NOT resolve conflicts
   - Do NOT reject combinations

Your output must ALWAYS be a valid QueryFacts JSON object.
Returning null values is ALWAYS allowed.

INTENT (what the user is asking for):
- POLICY_LOOKUP     → asking about a specific policy
- CATALOG_LIST      → listing projects, versions, items
- POLICY_LIST       → listing multiple policies
- EXPLAIN_TEXT      → conceptual explanation (no lookup)
- UNKNOWN            → intent cannot be determined

OPERATION (how the user phrases the request):
- LOOKUP
- LIST
- COUNT
- COMPARE
- EXPLAIN
- UNKNOWN

ENTITY (what object is referenced, if any):
- POLICY
- PROJECT
- VERSION
- REGION
- ADDRESS
- UNKNOWN

Intent extraction:
- Choose the intent that best matches the wording
- If unclear, use UNKNOWN

Operation extraction:
- Use LOOKUP if the query asks for details of one thing
- Use LIST if multiple items are requested
- Otherwise use UNKNOWN

Entity extraction:
- Extract only entities explicitly mentioned
- If multiple entities appear, choose the most central one
- If unclear, use UNKNOWN

Address handling rules:

- If a hexadecimal address like 0xEF01000 is present:
  - Convert it to a decimal integer
  - Set addr_start to that integer
  - Set addr_end to null

- If an address range is present (e.g. 0x1000–0x1FFF):
  - Convert both ends to decimal integers
  - Set addr_start and addr_end accordingly

- If no address is present:
  - addr_start = null
  - addr_end = null

Never fabricate addr_end.
Never expand a single address into a range.

- Extract project name only if explicitly stated
- Extract region only if explicitly stated
- Extract version only if explicitly stated
- Otherwise return null

Return ONLY a valid JSON object that conforms to the QueryFacts schema.
Do not include commentary, markdown, or explanations.
